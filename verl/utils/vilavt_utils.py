import re
import json
import traceback
import ast

from PIL import Image
from typing import List, Dict, Union
from qwen_vl_utils import fetch_image



REASONING_SYSTEM_PROMPT = """You are a helpful assistant. 
Your goal is to solve the problem in the provided image(s) based on the user's instruction. Proceed step by step, optionally using the zoom-in tool one or more times to examine key areas closely. Selected regions will be cropped and processed externally, then re-encoded with your query to extract critical details.

# Tools
If needed, use the zoom-in tool one or more times to examine specific areas in detail.

## Tool Format
Structure:
{
    "region": [
        {
            "index": int,       # Target image index to zoom in (0-based)
            "bbox_2d": list,    # Format: [x1, y1, x2, y2], where (x1, y1) is top-left corner and (x2, y2) is bottom-right corner
        },
        ...                    # Additional regions (optional)
    ],
    "query": str              # Description of what to look for in the selected regions
}

- Parameters:
    - region: List of dictionaries, each containing:
        - index: Integer, specifying which image to zoom in
        - bbox_2d: List of 4 integers [x1, y1, x2, y2] defining the region
    - query: String describing the search target

- Constraints:
    - At least one region must be specified
    - All coordinates must be within image boundaries
    - x1 < x2 and y1 < y2 must be satisfied

# Example:
<tool> {"region": [{"index": 0, "bbox_2d": [100, 200, 300, 400]}], "query": "Look for the red button"} </tool>
"""

SIMPLE_SYS_PROMPT = "You are a helpful assistant."

IMAGE_INDEX_PROMPT="""The index of the provided image is {index}.
"""

IMAGE_INDEX_PROMPT_V2="""The index of the zoom-in image is {current_image_idx} (width: {width}, height: {height}).
"""

# Answer the question using appropriate tools:
IMAGE_QUESTION_PROMPT="""This is an image indexed by 0. 
The image size: width {width}, height {height}.
"""

# multi-image may have different image size per image
MULTI_IMAGE_QUESTION_PROMPT="""These are {n_frames} images with indexed from 0 to {n_frames_1}. 
"""

VIDEO_QUESTION_PROMPT="""These are {n_frames} images with indexed from 0 to {n_frames_1}. 
All images have size: width {width}, height {height}.
"""


USER_PROMPT="""{image_instruction}
# Question: {question}

# If you need to zoom in for more details or examine specific regions, make tool call following the format:
<think> Your reasoning about where to look and why </think>
<tool> {{"region": [{{"index": int, "bbox_2d": [x1, y1, x2, y2]}}, ...], "query": str}} </tool>

# If you have enough information to answer the original question:
<think> Your reasoning here. </think>
<answer> Your final answer here. </answer>

- Note that x1, y1, x2, y2 are the coordinates of the bounding box in the specfied image by the index.
- You must strictly follow the above output format.
- In `<answer>`, provide **only** the final answer in the simplest required form:
  - For multiple-choice questions: output only the letter (e.g., `A`, `B`, `C`).
  - For yes/no questions: output only `Yes` or `No`.
  - For numerical answers: output only the number (e.g., `42`, `3.14`).
  - Do not include explanations, units, punctuation, or extra words.
"""

RESPONSE_PROMPT="""
# If you need to zoom in for more details or examine specific regions, make tool call following the format:
<think> Your reasoning about where to look and why. </think>
<tool> {{"region": [{{"index": , "bbox_2d": [x1, y1, x2, y2]}}, ...], "query": str}} </tool>

# If you have enough information to answer the original question:
<think> Your reasoning here. </think>
<answer> Your final answer here. </answer>
"""

RESPONSE_PROMPT_FINAL="""
You have reached the maximum number of iterations. **No further tool calls are allowed**.
You **must** summarize your reasoning and provide the final answer using the format below:
<think> Your reasoning here. </think>
<answer> Your final answer here. </answer>
"""

def parse_output(text: str) -> Dict or None:
    """
    Parses structured output from a model-generated text string.
    
    The function supports two types of structured tags:
        1. <tool>{...}</tool> — indicates a tool call (e.g., zoom-in request)
        2. <answer>{...}</answer> — contains the final answer in JSON/dict format
    
    Attempts to extract and parse content within these tags into a Python dictionary.
    
    Args:
        text (str): Raw output string generated by the model.
    
    Returns:
        dict or None: Parsed dictionary if successful; otherwise None.
    """
    # Check if the output contains a <tool> tag for external tool invocation
    if "<tool>" in text:
        pattern = r'<tool>(.*?)</tool>'
        match = re.search(pattern, text, re.DOTALL)  # re.DOTALL allows matching across lines
        if match:
            tool_content = match.group(1).strip()  # Extract inner content
            try:
                # Attempt to parse as JSON (standard format)
                tool_dict = json.loads(tool_content)
                return tool_dict
            except json.JSONDecodeError:
                # If JSON parsing fails, consider it invalid
                return None
        else:
            # No valid <tool>...</tool> block found
            return None
    else:
        # Otherwise, attempt to parse final answer wrapped in <answer> tags
        pattern = r'<answer>(.*?)</answer>'
        try:
            match = re.search(pattern, text, re.DOTALL)
            if not match:
                return None  # No <answer> tag found

            coords_str = match.group(1).strip()

            # First, try to parse as JSON
            try:
                answer_dict = json.loads(coords_str)
            except json.JSONDecodeError:
                # If JSON fails, try evaluating as a Python literal (e.g., dict syntax)
                try:
                    answer_dict = ast.literal_eval(coords_str)
                    assert isinstance(answer_dict, dict), "Parsed object is not a dictionary"
                except Exception:
                    return None  # All parsing attempts failed

            return answer_dict
        except Exception as e:
            # Log any unexpected error during parsing (useful for debugging)
            traceback.print_exc()
            return None


def zoomin(
    original_image_list: List[Image.Image],
    image_region_position: List[List[float]],
    original_image_url_list: List[str],
    tool_args: Dict,
    max_pixels: int | None = None,
) -> tuple:
    """
    Performs a "zoom-in" operation on specified regions of high-resolution source images.
    
    This function:
      - Reads the original high-res image from disk/URL
      - Maps low-resolution bounding boxes back to full-resolution coordinates
      - Crops the Region of Interest (ROI)
      - Resizes the cropped image with an intelligent scale factor
      - Applies pixel limits via `fetch_image` to ensure compatibility with downstream models
    
    Used in multi-step visual reasoning systems where the model requests closer inspection of areas.
    
    Args:
        original_image_list (List[PIL.Image]): Low-resolution images currently used as input.
        image_region_position (List[List[float]]): Normalized positions [x1,y1,x2,y2] of each image 
            relative to the original full-resolution image (used for coordinate mapping).
        original_image_url_list (List[str]): File paths or URLs to the original high-resolution images.
        tool_args (Dict): Parsed tool call dictionary containing:
            - 'region': List of dicts with 'index' and 'bbox_2d'
            - 'query': Text describing what to look for in the zoomed region
    
    Returns:
        tuple: (
            zoomin_image_list: List of zoomed/cropped PIL images,
            zoomin_region_position: Updated normalized coordinates of new crops,
            zoomin_image_url_list: Original image sources for each crop,
            query: String describing the analysis goal for the next step,
            valid_flag: Boolean indicating if the operation was successful
        )
    """
    # Initialize lists to store results
    zoomin_image_list, zoomin_region_position, zoomin_image_url_list = [], [], []
    valid_flag = True
    zoomin_feedback = ""

    try:
        if not isinstance(tool_args, dict):
            raise ValueError("tool_args must be a dictionary")

        if "region" not in tool_args:
            raise ValueError("Missing 'region' in tool_args")
        if "query" not in tool_args:
            raise ValueError("Missing 'query' in tool_args")

        query = str(tool_args["query"])

        for region in tool_args["region"]:
            if not isinstance(region, dict):
                raise ValueError(f"Invalid region format: {region}")
            if "index" not in region or "bbox_2d" not in region:
                raise ValueError(f"Region missing 'index' or 'bbox_2d': {region}")

            img_idx = region["index"]
            bbox = region["bbox_2d"]
            if not (isinstance(bbox, (list, tuple)) and len(bbox) == 4):
                raise ValueError(f"Invalid bbox format: {bbox}")

            x1, y1, x2, y2 = map(float, bbox)

            # Validate image index
            if not (0 <= img_idx < len(original_image_list)):
                raise ValueError(f"Image index {img_idx} out of range [0, {len(original_image_list)-1}]")

            orig_img = original_image_list[img_idx]
            width, height = orig_img.size
            ox1, oy1, ox2, oy2 = image_region_position[img_idx]

            # Validate bbox bounds
            if not (0 <= x1 < x2 <= width):
                raise ValueError(f"x-coordinates invalid: {x1}, {x2} for width {width}")
            if not (0 <= y1 < y2 <= height):
                raise ValueError(f"y-coordinates invalid: {y1}, {y2} for height {height}")

            # Map to normalized coordinates in original high-res image
            norm_x1 = ox1 + (x1 / width) * (ox2 - ox1)
            norm_y1 = oy1 + (y1 / height) * (oy2 - oy1)
            norm_x2 = ox1 + (x2 / width) * (ox2 - ox1)
            norm_y2 = oy1 + (y2 / height) * (oy2 - oy1)

            zoomin_region_position.append([norm_x1, norm_y1, norm_x2, norm_y2])

            # Load high-res image
            high_res_path = original_image_url_list[img_idx]
            high_res_img = Image.open(high_res_path)
            h_width, h_height = high_res_img.size

            # Convert normalized to absolute pixel coordinates
            abs_box = (
                int(norm_x1 * h_width),
                int(norm_y1 * h_height),
                int(norm_x2 * h_width),
                int(norm_y2 * h_height),
            )
            cropped = high_res_img.crop(abs_box)
            high_res_img.close()

            # Compute scale factor
            current_scale = width / ((ox2 - ox1) * h_width)
            scale = min(
                2 * current_scale,
                1 / max(norm_x2 - norm_x1, 1e-6),
                1 / max(norm_y2 - norm_y1, 1e-6)
            )

            new_size = (int((abs_box[2] - abs_box[0]) * scale), int((abs_box[3] - abs_box[1]) * scale))
            zoomed = cropped.resize(new_size, Image.Resampling.LANCZOS)

            # Apply pixel limit if needed
            fetch_dict = {"image": zoomed}
            if max_pixels is not None:
                fetch_dict["max_pixels"] = max_pixels
            zoomed = fetch_image(fetch_dict)

            zoomin_image_list.append(zoomed)
            zoomin_image_url_list.append(high_res_path)

        return zoomin_image_list, zoomin_region_position, zoomin_image_url_list, query, True

    except Exception as e:
        feedback = f"Zoom-in failed: {str(e)}"
        return [], [], [], feedback, False



def parse_dialogue(serialized_content: str):
    """
    解析 Qwen2.5-VL 的 chat template 输出
    
    Args:
        serialized_content: apply_chat_template 的输出
    
    Returns:
        对话消息列表
    """
    # 使用更精确的分割方式
    pattern = r'<\|im_start\|>(.*?)<\|im_end\|>'
    matches = re.findall(pattern, serialized_content, re.DOTALL)
    
    conversations = []
    
    for match in matches:
        match = match.strip()
        
        if not match:
            continue
        
        # 分离角色和内容（只在第一个换行符处分割）
        lines = match.split('\n', 1)
        if len(lines) < 2:
            print(f"⚠ Skipping invalid segment: {match[:50]}...")
            continue
        
        role = lines[0].strip()
        content_text = lines[1] if len(lines) > 1 else ""
        
        # 处理 system
        if role == 'system':
            conversations.append({
                "role": "system",
                "content": content_text
            })
        
        # 处理 user
        elif role == 'user':
            content = []
            
            # 检测并提取图像标记
            vision_pattern = r'<\|vision_start\|><\|image_pad\|><\|vision_end\|>'
            
            # 分割文本和图像标记
            parts = re.split(vision_pattern, content_text)
            vision_matches = re.findall(vision_pattern, content_text)
            
            # 交替添加图像和文本
            for i, part in enumerate(parts):
                # 添加图像（如果前面有图像标记）
                if i > 0 and i - 1 < len(vision_matches):
                    content.append({
                        "type": "image",
                        # 注意：无法从 template 恢复具体图像信息
                    })
                
                # 添加文本（如果非空）
                if part.strip():
                    content.append({
                        "type": "text",
                        "text": part.strip()
                    })
            
            # 如果 content 为空，至少添加一个空文本
            if not content:
                content.append({"type": "text", "text": ""})
            
            conversations.append({
                "role": "user",
                "content": content
            })
        
        # 处理 assistant
        elif role == 'assistant':
            conversations.append({
                "role": "assistant",
                "content": content_text
            })
        
        else:
            print(f"⚠ Unknown role: {role}")
    
    return conversations
