import re
import json
import traceback
import ast

from PIL import Image
from typing import List, Dict, Union
from qwen_vl_utils import fetch_image

REASONING_SYS_PROMPT="""You are a helpful assistant. 
Your goal is to solve the problem in the provided image(s) based on the user's instruction. Proceed step by step, optionally using the zoom-in tool one or more times to examine key areas closely. Selected regions will be cropped and processed externally, then re-encoded with your query to extract critical details.

# Tools
If needed, use the zoom-in tool one or more times to examine specific areas in detail.

## Tool Format
Structure:
{
    "region": [
        {
            "index": int,       # Target image index to zoom in (0-based)
            "bbox_2d": list,    # Format: [x1, y1, x2, y2], where (x1, y1) is top-left corner and (x2, y2) is bottom-right corner
        },
        ...                    # Additional regions (optional)
    ],
    "query": str              # Description of what to look for in the selected regions
}

- Parameters:
    - region: List of dictionaries, each containing:
        - index: Integer, specifying which image to zoom in
        - bbox_2d: List of 4 integers [x1, y1, x2, y2] defining the region
    - query: String describing the search target

- Constraints:
    - At least one region must be specified
    - All coordinates must be within image boundaries
    - x1 < x2 and y1 < y2 must be satisfied

# Example:
<tool> {"region": [{"index": 0, "bbox_2d": [100, 200, 300, 400]}], "query": "Look for the red button"} </tool>
"""


SIMPLE_SYS_PROMPT = "You are a helpful assistant."

IMAGE_INDEX_PROMPT="""The index of the provided image is {index}.
"""


IMAGE_INDEX_PROMPT_V1="""The index of the provided image is {current_image_idx} (width: {width}, height: {height}).
"""

IMAGE_INDEX_PROMPT_V2="""The index of the zoom-in image is {current_image_idx} (width: {width}, height: {height}).
"""

# Answer the question using appropriate tools:
IMAGE_QUESTION_PROMPT="""This is an image indexed by 0. 
The image size: width {width}, height {height}.
"""

# multi-image may have different image size per image
MULTI_IMAGE_QUESTION_PROMPT="""These are {n_frames} images with indexed from 0 to {n_frames_1}. 
"""

# video frames have the same image size
VIDEO_QUESTION_PROMPT="""These are {n_frames} images with indexed from 0 to {n_frames_1}. 
All images have size: width {width}, height {height}.
"""

USER_PROMPT="""{image_instruction}
{question}

# If you need to zoom in for more details or examine specific regions, make tool call following the format:
<think> Your reasoning about where to look and why </think>
<tool> {{"region": [{{"index": int, "bbox_2d": [x1, y1, x2, y2]}}, ...], "query": str}} </tool>

# If you have enough information to answer the original question:
<think> Your reasoning here. </think>
<answer> Your final answer here. </answer>

- Note that x1, y1, x2, y2 are the coordinates of the bounding box in the specfied image by the index.
- You must strictly follow the above output format. 
- In `<answer>`, provide **only** the final answer in the simplest required form:
   - For multiple-choice questions: output only the letter (e.g., `A`, `B`, `C`).
   - Do not include explanations, units, punctuation, or extra words.
"""



RESPONSE_PROMPT="""
# If you need to zoom in for more details or examine specific regions, make tool call following the format:
<think> Your reasoning about where to look and why. </think>
<tool> {{"region": [{{"index": , "bbox_2d": [x1, y1, x2, y2]}}, ...], "query": str}} </tool>

# If you have enough information to answer the original question:
<think> Your reasoning here. </think>
<answer> Your final answer here. </answer>
"""

RESPONSE_PROMPT_FINAL="""
You have reached the maximum number of iterations. **No further tool calls are allowed**.
You **must** summarize your reasoning and provide the final answer using the format below:
<think> Your reasoning here. </think>
<answer> Your final answer here. </answer>
"""


def parse_output(text: str) -> Dict or None:
    """
    Parses structured output from a model-generated text string.
    
    The function supports two types of structured tags:
        1. <tool>{...}</tool> — indicates a tool call (e.g., zoom-in request)
        2. <answer>{...}</answer> — contains the final answer in JSON/dict format
    
    Attempts to extract and parse content within these tags into a Python dictionary.
    
    Args:
        text (str): Raw output string generated by the model.
    
    Returns:
        dict or None: Parsed dictionary if successful; otherwise None.
    """
    # Check if the output contains a <tool> tag for external tool invocation
    if "<tool>" in text:
        pattern = r'<tool>(.*?)</tool>'
        match = re.search(pattern, text, re.DOTALL)  # re.DOTALL allows matching across lines
        if match:
            tool_content = match.group(1).strip()  # Extract inner content
            try:
                # Attempt to parse as JSON (standard format)
                tool_dict = json.loads(tool_content)
                return tool_dict
            except json.JSONDecodeError:
                # If JSON parsing fails, consider it invalid
                return None
        else:
            # No valid <tool>...</tool> block found
            return None
    else:
        # Otherwise, attempt to parse final answer wrapped in <answer> tags
        pattern = r'<answer>(.*?)</answer>'
        try:
            match = re.search(pattern, text, re.DOTALL)
            if not match:
                return None  # No <answer> tag found

            coords_str = match.group(1).strip()

            # First, try to parse as JSON
            try:
                answer_dict = json.loads(coords_str)
            except json.JSONDecodeError:
                # If JSON fails, try evaluating as a Python literal (e.g., dict syntax)
                try:
                    answer_dict = ast.literal_eval(coords_str)
                    assert isinstance(answer_dict, dict), "Parsed object is not a dictionary"
                except Exception:
                    return None  # All parsing attempts failed

            return answer_dict
        except Exception as e:
            # Log any unexpected error during parsing (useful for debugging)
            traceback.print_exc()
            return None


def zoomin(
    original_image_list: List[Image.Image],
    image_region_position: List[List[float]],
    original_image_url_list: List[str],
    tool_args: Dict,
    max_pixels: int | None = None,
) -> tuple:
    """
    Performs a "zoom-in" operation on specified regions of high-resolution source images.
    
    This function:
      - Reads the original high-res image from disk/URL
      - Maps low-resolution bounding boxes back to full-resolution coordinates
      - Crops the Region of Interest (ROI)
      - Resizes the cropped image with an intelligent scale factor
      - Applies pixel limits via `fetch_image` to ensure compatibility with downstream models
    
    Used in multi-step visual reasoning systems where the model requests closer inspection of areas.
    
    Args:
        original_image_list (List[PIL.Image]): Low-resolution images currently used as input.
        image_region_position (List[List[float]]): Normalized positions [x1,y1,x2,y2] of each image 
            relative to the original full-resolution image (used for coordinate mapping).
        original_image_url_list (List[str]): File paths or URLs to the original high-resolution images.
        tool_args (Dict): Parsed tool call dictionary containing:
            - 'region': List of dicts with 'index' and 'bbox_2d'
            - 'query': Text describing what to look for in the zoomed region
    
    Returns:
        tuple: (
            zoomin_image_list: List of zoomed/cropped PIL images,
            zoomin_region_position: Updated normalized coordinates of new crops,
            zoomin_image_url_list: Original image sources for each crop,
            query: String describing the analysis goal for the next step,
            valid_flag: Boolean indicating if the operation was successful
        )
    """
    # Initialize lists to store results
    zoomin_image_list, zoomin_region_position, zoomin_image_url_list = [], [], []
    valid_flag = True
    zoomin_feedback = ""

    try:
        if not isinstance(tool_args, dict):
            raise ValueError("tool_args must be a dictionary")

        if "region" not in tool_args:
            raise ValueError("Missing 'region' in tool_args")
        if "query" not in tool_args:
            raise ValueError("Missing 'query' in tool_args")

        query = str(tool_args["query"])

        for region in tool_args["region"]:
            if not isinstance(region, dict):
                raise ValueError(f"Invalid region format: {region}")
            if "index" not in region or "bbox_2d" not in region:
                raise ValueError(f"Region missing 'index' or 'bbox_2d': {region}")

            img_idx = region["index"]
            bbox = region["bbox_2d"]
            if not (isinstance(bbox, (list, tuple)) and len(bbox) == 4):
                raise ValueError(f"Invalid bbox format: {bbox}")

            x1, y1, x2, y2 = map(float, bbox)

            # Validate image index
            if not (0 <= img_idx < len(original_image_list)):
                raise ValueError(f"Image index {img_idx} out of range [0, {len(original_image_list)-1}]")

            orig_img = original_image_list[img_idx]
            width, height = orig_img.size
            ox1, oy1, ox2, oy2 = image_region_position[img_idx]

            # Validate bbox bounds
            if not (0 <= x1 < x2 <= width):
                raise ValueError(f"x-coordinates invalid: {x1}, {x2} for width {width}")
            if not (0 <= y1 < y2 <= height):
                raise ValueError(f"y-coordinates invalid: {y1}, {y2} for height {height}")

            # Map to normalized coordinates in original high-res image
            norm_x1 = ox1 + (x1 / width) * (ox2 - ox1)
            norm_y1 = oy1 + (y1 / height) * (oy2 - oy1)
            norm_x2 = ox1 + (x2 / width) * (ox2 - ox1)
            norm_y2 = oy1 + (y2 / height) * (oy2 - oy1)

            zoomin_region_position.append([norm_x1, norm_y1, norm_x2, norm_y2])

            # Load high-res image
            high_res_path = original_image_url_list[img_idx]
            high_res_img = Image.open(high_res_path)
            h_width, h_height = high_res_img.size

            # Convert normalized to absolute pixel coordinates
            abs_box = (
                int(norm_x1 * h_width),
                int(norm_y1 * h_height),
                int(norm_x2 * h_width),
                int(norm_y2 * h_height),
            )
            cropped = high_res_img.crop(abs_box)
            high_res_img.close()

            # Compute scale factor
            current_scale = width / ((ox2 - ox1) * h_width)
            scale = min(
                2 * current_scale,
                1 / max(norm_x2 - norm_x1, 1e-6),
                1 / max(norm_y2 - norm_y1, 1e-6)
            )

            new_size = (int((abs_box[2] - abs_box[0]) * scale), int((abs_box[3] - abs_box[1]) * scale))
            zoomed = cropped.resize(new_size, Image.Resampling.LANCZOS)

            # Apply pixel limit if needed
            fetch_dict = {"image": zoomed}
            if max_pixels is not None:
                fetch_dict["max_pixels"] = max_pixels
            zoomed = fetch_image(fetch_dict)

            zoomin_image_list.append(zoomed)
            zoomin_image_url_list.append(high_res_path)

        return zoomin_image_list, zoomin_region_position, zoomin_image_url_list, query, True

    except Exception as e:
        feedback = f"Zoom-in failed: {str(e)}"
        return [], [], [], feedback, False


def generate_prompt_simple_qa(user_question):
    # Construct the prompt based on the given requirements
    # "Keep your response concise and factual.\n"
    prompt = f'''You are an advanced AI assistant specialized in Visual Question Answering (VQA).
Your task is to analyze the provided image(s) and answer the user's question based on visual content.

**User's Question:** "{user_question}"

Output format:
<think> Your reasoning here. </think>
<answer> Your final answer here. </answer>

- You must strictly follow the above output format.
- In `<answer>`, provide **only** the final answer:
  - Multiple-choice: `A`, `B`, etc.
  - Yes/no: `Yes` or `No`
  - Numerical: `42`, `3.14`, etc.
  - No explanations, units, punctuation, or extra text.
'''
    return prompt


def generate_prompt_qa(user_question, user_image_path_list, max_pixels, min_pixels):
    # Construct the prompt based on the given requirements
    try:
        fetch_image_dict = {"image": user_image_path_list[-1]}
        if max_pixels is not None:
            fetch_image_dict['max_pixels'] = max_pixels
        if min_pixels is not None:
            fetch_image_dict['min_pixels'] = min_pixels
        img = fetch_image(fetch_image_dict)
        width, height = img.size
    except Exception:
        width, height = None, None
    
    if len(user_image_path_list) > 1:
        # each video frame has the same size
        image_instruction =  MULTI_IMAGE_QUESTION_PROMPT.format(n_frames=len(user_image_path_list), n_frames_1=len(user_image_path_list)-1) 
    else:
        # image_instruction =  IMAGE_QUESTION_PROMPT.format(width=width, height=height) 
        image_instruction =  ""
        
    prompt =  USER_PROMPT.format(image_instruction=image_instruction, question=user_question)
    return prompt

