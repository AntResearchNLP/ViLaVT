#!/usr/bin/env python
# eval_vilavt.py - ViLaVT Model Evaluator with Distributed Support

import os
import sys
import json
import pickle
import argparse
import glob


import torch
from torch.utils.data import Dataset, DataLoader
from torch.utils.data.distributed import DistributedSampler
from torch import distributed as dist
from tqdm import tqdm
import subprocess
from datetime import timedelta


# === å·¥å…·å‡½æ•°ï¼šdump / loadï¼ˆå…¼å®¹ smpï¼‰===
def dump(obj, filepath, **kwargs):
    """Pickle dump with mkdir support"""
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    with open(filepath, 'wb') as f:
        pickle.dump(obj, f)

def load(filepath, **kwargs):
    """Pickle load"""
    with open(filepath, 'rb') as f:
        return pickle.load(f)


# === å…¨å±€å¸¸é‡ ===
TYPE_TEMPLATE = {
    "multiple choice": '\nAnswer with the option\'s letter from the given choices directly.',
    "free-form": '',
    "regression": '\nPlease answer the question using a single word or phrase (e.g., 42 or 3.14).',
    "numerical": '\nPlease answer the question using a single word or phrase (e.g., 42 or 3.14).',
}

# === åˆ†å¸ƒå¼åˆå§‹åŒ–ï¼ˆå…¼å®¹å•è¿›ç¨‹ï¼‰===
def setup_distributed():
    if 'RANK' in os.environ:
        if not dist.is_initialized():
            dist.init_process_group(
            backend="nccl",
            timeout=timedelta(hours=2)              # âœ… å¢åŠ åˆ° 2 å°æ—¶
        )

        world_size = dist.get_world_size()
        rank = dist.get_rank()
        local_rank = int(os.environ.get("LOCAL_RANK", 0))
        is_distributed = True
    else:
        world_size = 1
        rank = 0
        local_rank = 0
        is_distributed = False
    torch.cuda.set_device(local_rank)
    return local_rank, rank, world_size, is_distributed


# === æ•°æ®é›†å°è£… ===
class EvalDataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx], idx


# === æ„å»º prompt å’Œæ¶ˆæ¯ ===
# def build_message(item, image_folder="", add_type_template=True):
#     prompt = item['question']
#     ptype = item['problem_type'].lower()

#     if ptype == 'multiple choice' and  item.get('options', None) is not None:
#         prompt += '\n' + '\n'.join(item['options'])
#     if add_type_template:
#         prompt += TYPE_TEMPLATE.get(ptype, "")

#     message = []
#     for img_path in item['images']:
#         full_path = os.path.join(image_folder, img_path) if image_folder else img_path
#         message.append({'type': 'image', 'value': full_path})
#     message.append({'type': 'text', 'value': prompt})
#     return message


def build_message(item, image_folder="", add_type_template=True):
    """
    æ„å»ºæ¶ˆæ¯ï¼Œæ”¯æŒåœ¨promptä¸­æŒ‰<image>æ ‡ç­¾ä½ç½®æ’å…¥å›¾ç‰‡
    
    Args:
        item: åŒ…å«question, images, problem_typeç­‰å­—æ®µçš„å­—å…¸
        image_folder: å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„
        add_type_template: æ˜¯å¦æ·»åŠ ç±»å‹æ¨¡æ¿
        
    Returns:
        message: åŒ…å«textå’Œimageçš„æ¶ˆæ¯åˆ—è¡¨
    """
    prompt = item['question']
    ptype = item['problem_type'].lower()
    
    # æ·»åŠ é€‰é¡¹
    if ptype == 'multiple choice' and item.get('options', None) is not None:
        prompt += '\n' + '\n'.join(item['options'])
    
    # æ·»åŠ ç±»å‹æ¨¡æ¿
    if add_type_template:
        prompt += TYPE_TEMPLATE.get(ptype, "")
    
    message = []
    
    # æ£€æŸ¥promptä¸­æ˜¯å¦åŒ…å«<image>æ ‡ç­¾
    if '<image>' in prompt:
        # æŒ‰<image>æ ‡ç­¾åˆ†å‰²prompt
        parts = prompt.split('<image>')
        
        # ç¡®ä¿å›¾ç‰‡æ•°é‡ä¸<image>æ ‡ç­¾æ•°é‡åŒ¹é…
        num_image_tags = prompt.count('<image>')
        num_images = len(item['images'])
        
        if num_image_tags != num_images:
            print(f"Warning: Number of <image> tags ({num_image_tags}) != number of images ({num_images})")
            # å…œåº•ç­–ç•¥ï¼šå›¾ç‰‡æ•°é‡ä¸åŒ¹é…æ—¶ï¼Œä½¿ç”¨é»˜è®¤æ–¹å¼ï¼ˆæ‰€æœ‰å›¾ç‰‡åœ¨å‰ï¼‰
            for img_path in item['images']:
                full_path = os.path.join(image_folder, img_path) if image_folder else img_path
                message.append({'type': 'image', 'value': full_path})
            message.append({'type': 'text', 'value': prompt.replace('<image>', '')})
            return message
        
        # äº¤ç»‡æ’å…¥æ–‡æœ¬å’Œå›¾ç‰‡
        for i, part in enumerate(parts):
            # æ·»åŠ æ–‡æœ¬éƒ¨åˆ†ï¼ˆå¦‚æœéç©ºï¼‰
            if part:
                message.append({'type': 'text', 'value': part})
            
            # æ·»åŠ å›¾ç‰‡ï¼ˆæœ€åä¸€ä¸ªéƒ¨åˆ†åé¢æ²¡æœ‰å›¾ç‰‡ï¼‰
            if i < len(item['images']):
                full_path = os.path.join(image_folder, img_path) if image_folder else item['images'][i]
                message.append({'type': 'image', 'value': full_path})
    
    else:
        # æ— <image>æ ‡ç­¾ï¼šé»˜è®¤æ‰€æœ‰å›¾ç‰‡åœ¨å‰é¢
        for img_path in item['images']:
            full_path = os.path.join(image_folder, img_path) if image_folder else img_path
            message.append({'type': 'image', 'value': full_path})
        message.append({'type': 'text', 'value': prompt})
    
    return message

# === ä¸»è¦è¯„æµ‹å‡½æ•° ===
def eval_model(args):
    local_rank, rank, world_size, is_distributed = setup_distributed()
    print(f"local_rank, rank, world_size, is_distributed: {local_rank, rank, world_size, is_distributed}")
    # --- è®¾å¤‡ä¸æ¨¡å‹ ---
    device = torch.device(f"cuda:{local_rank}")
    # torch.cuda.set_device(device)

    if args.use_baseline:
        from vilavt_baseline import ViLaVT_Baseline as ViLaVT
    else:
        # from vilavt import ViLaVT
        from vilavt_rl import ViLaVT
    
    
    # --- å‚æ•°è§£æ ---
    model_path = args.model_path
    model_name = args.model_name
    input_file = args.input_file
    dataset_name = args.dataset
    output_dir = args.output_dir
    image_folder = args.image_folder if args.image_folder not in ["None", ""] else ""

    # --- æ„å»ºè·¯å¾„ ---
    model_output_dir = os.path.join(output_dir, model_name)
    os.makedirs(model_output_dir, exist_ok=True)

    # æœ€ç»ˆç»“æœæ–‡ä»¶
    final_result_file = os.path.join(model_output_dir, f"{model_name}_{dataset_name}_results.jsonl")

    # å¦‚æœç»“æœå·²å­˜åœ¨ï¼Œè·³è¿‡ï¼ˆæ”¯æŒ reuseï¼‰
    if args.reuse and os.path.exists(final_result_file):
        if rank == 0:
            print(f"âœ… Result already exists: {final_result_file}, skipping inference.")
        return

    # ä¸´æ—¶ä¸­é—´æ–‡ä»¶ï¼štmp_{rank}_{world_size}_xxx.pkl
    tmp_result_file = os.path.join(
        model_output_dir,
        f"tmp_{rank}_{world_size}_{model_name}_{dataset_name}.pkl"
    )

    # --- åŠ è½½æ•°æ® ---
    if input_file.endswith('.jsonl'):
        with open(input_file, 'r', encoding='utf-8') as f:
            data = [json.loads(line) for line in f]
    else:
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
    data=data

    model = ViLaVT(
        model_path=model_path,
        max_iterations=2,
        post_process=True,
        # measure_latency=True,  # âœ… å¯ç”¨
        verbose=(rank == 0),
        device=device,
        max_pixels=args.max_pixels,
    )
    model.model.eval()

    # --- æ•°æ®åˆ†ç‰‡ ---
    dataset = EvalDataset(data)
    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=False) if is_distributed else None
    dataloader = DataLoader(dataset, batch_size=None, sampler=sampler, num_workers=0)

    # --- æ¨ç† ---
    predictions = {}  # idx -> { 'response': ..., 'conversations': ... }

    pbar_desc = f"Rank {rank}/{world_size}"
    total_samples = len(dataloader)

    for (item, original_idx) in tqdm(
        dataloader,
        desc=pbar_desc,
        total=total_samples,
        position=rank if is_distributed else 0,                    # æ¯ä¸ª rank ç‹¬ç«‹è¡Œ
        leave=(rank == 0),                     # å®Œæˆåæ¸…é™¤è¿›åº¦æ¡ï¼ˆæ•´æ´ï¼‰
        ncols=100,                        # å›ºå®šå®½åº¦
        file=sys.stdout,                  # å¿…é¡»æŒ‡å®šï¼Œå¦åˆ™å¯èƒ½ä¸æ˜¾ç¤º
        disable=(rank != 0 and is_distributed)  # åˆ†å¸ƒå¼æ—¶åªæœ‰ rank 0 æ˜¾ç¤ºï¼ŸNOï¼æˆ‘ä»¬æƒ³çœ‹æ‰€æœ‰
    ):
        idx = int(original_idx)
        # try:
        add_type_template = not (dataset_name in ["ERQA"])
        message = build_message(item, image_folder, add_type_template)
        response, conversations= model.generate_inner(message, dataset_name)
        idx = int(original_idx)
        predictions[idx] = {
                'response': response,
                'conversations': conversations  # ä¿å­˜å¯¹è¯å†å²ï¼ˆå¦‚æœéœ€è¦ï¼‰
        }

    # --- ä¿å­˜ä¸­é—´ç»“æœ ---
    dump(predictions, tmp_result_file)
    if rank == 0:
        print(f"ğŸ’¾ Saved temporary results to {tmp_result_file}")

    # --- åŒæ­¥ä¸åˆå¹¶ ---
    if world_size > 1:
        print(f"[Rank {rank}] Waiting at barrier...")
        dist.barrier(device_ids=[local_rank])
        if rank == 0:
            merged = {}
            pattern = os.path.join(model_output_dir, f"tmp_*_{world_size}_{model_name}_{dataset_name}.pkl")
            for pkl_file in glob.glob(pattern):
                part = load(pkl_file)
                merged.update(part)
                os.remove(pkl_file)

            # æŒ‰ index æ’åºå†™å…¥æœ€ç»ˆ JSONL
            with open(final_result_file, 'w', encoding='utf-8') as fout:
                for idx in sorted(merged.keys()):
                    item = data[idx].copy()  # é¿å…ä¿®æ”¹åŸå§‹ data
                    pred = merged[idx]
                    item['response'] = pred['response']
                    item['model_id'] = model_name
                    
                    # å¯é€‰ï¼šä¿å­˜ä¸­é—´ä¿¡æ¯ï¼ˆç”¨äºåˆ†æï¼‰
                    if args.save_intermediate:  # æ–°å¢ä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°æ§åˆ¶
                        item['conversations'] = pred['conversations']
                    
                    fout.write(json.dumps(item, ensure_ascii=False) + "\n")

            print(f"ğŸ‰ Final results saved to {final_result_file}")

        dist.barrier(device_ids=[local_rank])
        dist.destroy_process_group()
    else:
        # å•è¿›ç¨‹æ¨¡å¼
        merged = predictions
        with open(final_result_file, 'w', encoding='utf-8') as fout:
            for idx in sorted(merged.keys()):
                item = data[idx]
                item['response'] = merged[idx]['response']
                item['conversations'] = merged[idx]['conversations']
                item['model_id'] = model_name
                fout.write(json.dumps(item, ensure_ascii=False) + "\n")
        if os.path.exists(tmp_result_file):
            os.remove(tmp_result_file)
        print(f"âœ… Debug mode: Results saved to {final_result_file}")


# === å‚æ•°è§£æå…¥å£ ===
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate ViLaVT model with distributed support.")
    parser.add_argument("--model-path", type=str, required=True)
    parser.add_argument("--model-name", type=str, required=True)
    parser.add_argument("--dataset", type=str, required=True)
    parser.add_argument("--input-file", type=str, required=True)
    parser.add_argument("--image-folder", type=str, default="")
    parser.add_argument("--output-dir", type=str, default="./outputs")
    parser.add_argument("--use-baseline", action="store_true")
    parser.add_argument("--use-deepeyes", action="store_true")
    parser.add_argument('--save-intermediate', action='store_true', help='Save input message and conversations in final output')
    parser.add_argument("--reuse", action="store_true")
    parser.add_argument("--max-pixels", type=int, default=8192*28*28)
    args = parser.parse_args()

    eval_model(args)

